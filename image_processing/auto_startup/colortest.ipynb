{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Segmentation Routine (using k-means clustering)\n",
    "# ========================================================\n",
    "def segment_two_colors(frame, k=2):\n",
    "    \"\"\"\n",
    "    Given a frame that has two dominant color regions, segment it using k-means clustering.\n",
    "    Assumes that one cluster is the object (smaller area) and the other is the background.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "      - object_mask, background_mask: binary masks for each region.\n",
    "      - object_color_lab, background_color_lab: cluster centers in LAB color space.\n",
    "      - object_color_bgr, background_color_bgr: cluster centers converted to BGR.\n",
    "      - object_area, background_area: pixel counts for each cluster.\n",
    "      - object_bbox: bounding box (x, y, w, h) for the object (largest contour in object mask).\n",
    "    \"\"\"\n",
    "    # Convert image to LAB color space (LAB tends to be perceptually uniform)\n",
    "    lab_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # Reshape into 2D array of pixels and convert to float32 for k-means.\n",
    "    pixel_values = lab_frame.reshape((-1, 3))\n",
    "    pixel_values = np.float32(pixel_values)\n",
    "    \n",
    "    # Define k-means criteria and run clustering.\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    attempts = 10\n",
    "    ret, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, attempts, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Reshape labels back to the image shape.\n",
    "    labels = labels.flatten()\n",
    "    segmented_img = labels.reshape(frame.shape[:2])\n",
    "    \n",
    "    # Determine the area (pixel count) of each cluster.\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_areas = dict(zip(unique, counts))\n",
    "    \n",
    "    # Assume that the background is the cluster with more pixels.\n",
    "    background_index = max(cluster_areas, key=cluster_areas.get)\n",
    "    object_index     = min(cluster_areas, key=cluster_areas.get)\n",
    "    \n",
    "    # Create binary masks.\n",
    "    object_mask = np.uint8((segmented_img == object_index)) * 255\n",
    "    background_mask = np.uint8((segmented_img == background_index)) * 255\n",
    "    \n",
    "    # Retrieve the cluster centers (in LAB).\n",
    "    object_color_lab     = centers[object_index]\n",
    "    background_color_lab = centers[background_index]\n",
    "    \n",
    "    # Convert LAB centers to BGR for easier visualization.\n",
    "    object_color_bgr = cv2.cvtColor(np.uint8([[object_color_lab]]), cv2.COLOR_LAB2BGR)[0][0]\n",
    "    background_color_bgr = cv2.cvtColor(np.uint8([[background_color_lab]]), cv2.COLOR_LAB2BGR)[0][0]\n",
    "    \n",
    "    # Compute a bounding box for the object based on its mask.\n",
    "    contours, _ = cv2.findContours(object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        object_bbox = cv2.boundingRect(largest_contour)\n",
    "    else:\n",
    "        object_bbox = None\n",
    "    \n",
    "    result = {\n",
    "        'object_mask': object_mask,\n",
    "        'background_mask': background_mask,\n",
    "        'object_color_lab': object_color_lab,\n",
    "        'background_color_lab': background_color_lab,\n",
    "        'object_color_bgr': object_color_bgr,\n",
    "        'background_color_bgr': background_color_bgr,\n",
    "        'object_area': cluster_areas[object_index],\n",
    "        'background_area': cluster_areas[background_index],\n",
    "        'object_bbox': object_bbox\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def mask_pixels_within_lab_range(frame, lab_colors, tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Given an input frame (BGR) and a set of reference LAB colors, return a binary mask where\n",
    "    each pixel is selected if its LAB value is within ±(tolerance*100)% of any of the reference colors.\n",
    "    \n",
    "    For each reference color, the tolerance is applied per channel. For example, if the reference\n",
    "    LAB value is [L, a, b], then a pixel's LAB value [L_p, a_p, b_p] is considered close if:\n",
    "      L_p is in [L*(1-tolerance), L*(1+tolerance)],\n",
    "      a_p is in [a*(1-tolerance), a*(1+tolerance)], and\n",
    "      b_p is in [b*(1-tolerance), b*(1+tolerance)].\n",
    "    \n",
    "    Parameters:\n",
    "      frame (numpy.ndarray): Input image in BGR format.\n",
    "      lab_colors (list or tuple): A list of reference LAB colors. Each color should be an iterable\n",
    "                                  of three values (L, a, b). The values should be in the same scale\n",
    "                                  as produced by cv2.cvtColor(..., cv2.COLOR_BGR2LAB) (typically 0–255).\n",
    "      tolerance (float): Relative tolerance (default 0.1 for 10%).\n",
    "    \n",
    "    Returns:\n",
    "      mask (numpy.ndarray): A binary mask (uint8) where matching pixels are 255 and others 0.\n",
    "    \"\"\"\n",
    "    # Convert the input frame from BGR to LAB color space.\n",
    "    lab_img = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # Initialize an empty mask (all zeros).\n",
    "    mask_total = np.zeros(lab_img.shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    for color in lab_colors:\n",
    "        # Convert the reference color to a float32 NumPy array.\n",
    "        ref = np.array(color, dtype=np.float32)\n",
    "        \n",
    "        # Compute lower and upper bounds as ±(tolerance * value) per channel.\n",
    "        lower_bound = np.clip(ref * (1 - tolerance), 0, 255).astype(np.uint8)\n",
    "        upper_bound = np.clip(ref * (1 + tolerance), 0, 255).astype(np.uint8)\n",
    "        print(lower_bound)\n",
    "        print(upper_bound)\n",
    "        # Generate a mask for pixels within these bounds.\n",
    "        mask = cv2.inRange(lab_img, lower_bound, upper_bound)\n",
    "        \n",
    "        # Combine the mask with the total mask (bitwise OR).\n",
    "        mask_total = cv2.bitwise_or(mask_total, mask)\n",
    "    \n",
    "    return mask_total\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# Segmentation Model Class\n",
    "# ========================================================\n",
    "class SegmentationModel:\n",
    "    def __init__(self, object_color_lab, background_color_lab, object_area, background_area, object_bbox):\n",
    "        self.object_color_lab = np.array(object_color_lab, dtype=np.float32)\n",
    "        self.background_color_lab = np.array(background_color_lab, dtype=np.float32)\n",
    "        self.object_area = object_area\n",
    "        self.background_area = background_area\n",
    "        self.object_bbox = object_bbox\n",
    "\n",
    "    def update(self, new_object_color_lab, new_background_color_lab, new_object_area, new_background_area, new_object_bbox, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Update the stored cluster centers (and other parameters) using a weighted moving average.\n",
    "        alpha is the update rate (0 < alpha < 1).\n",
    "        \"\"\"\n",
    "        self.object_color_lab = (1 - alpha) * self.object_color_lab + alpha * np.array(new_object_color_lab, dtype=np.float32)\n",
    "        self.background_color_lab = (1 - alpha) * self.background_color_lab + alpha * np.array(new_background_color_lab, dtype=np.float32)\n",
    "        self.object_area = new_object_area  # Optionally, you could update this with a moving average as well.\n",
    "        self.background_area = new_background_area\n",
    "        self.object_bbox = new_object_bbox\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the model to disk using pickle.\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        \"\"\"Load the model from disk using pickle.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        return model\n",
    "\n",
    "# ========================================================\n",
    "# Helper Functions to Initialize and Update the Model\n",
    "# ========================================================\n",
    "def initialize_segmentation_model(frame, k=2):\n",
    "    \"\"\"\n",
    "    Run the segmentation routine on the frame and initialize a SegmentationModel.\n",
    "    \"\"\"\n",
    "    result = segment_two_colors(frame, k=k)\n",
    "    model = SegmentationModel(\n",
    "        object_color_lab=result['object_color_lab'],\n",
    "        background_color_lab=result['background_color_lab'],\n",
    "        object_area=result['object_area'],\n",
    "        background_area=result['background_area'],\n",
    "        object_bbox=result['object_bbox']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def update_segmentation_model(frame, model, k=2, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Update an existing segmentation model with a new frame.\n",
    "    \n",
    "    Since the ordering of clusters might change between runs of k-means, we\n",
    "    compare the new centers with the stored ones in the model and assign them accordingly.\n",
    "    \"\"\"\n",
    "    result = segment_two_colors(frame, k=k)\n",
    "    new_obj_lab = result['object_color_lab']\n",
    "    new_bg_lab = result['background_color_lab']\n",
    "    \n",
    "    # Compare new centers with stored object center.\n",
    "    # Compute distances in LAB space.\n",
    "    d_obj_to_new_obj = np.linalg.norm(model.object_color_lab - new_obj_lab)\n",
    "    d_obj_to_new_bg  = np.linalg.norm(model.object_color_lab - new_bg_lab)\n",
    "    \n",
    "    # If the new cluster labeled as \"object\" is farther from our stored object center than\n",
    "    # the new cluster labeled as \"background\", then swap them.\n",
    "    if d_obj_to_new_bg < d_obj_to_new_obj:\n",
    "        new_obj_lab, new_bg_lab = new_bg_lab, new_obj_lab\n",
    "        new_obj_area, new_bg_area = result['background_area'], result['object_area']\n",
    "        new_obj_bbox = result['object_bbox']  # Assuming object_bbox always corresponds to the object cluster.\n",
    "    else:\n",
    "        new_obj_area, new_bg_area = result['object_area'], result['background_area']\n",
    "        new_obj_bbox = result['object_bbox']\n",
    "    \n",
    "    # Update the model using a weighted average update.\n",
    "    model.update(new_obj_lab, new_bg_lab, new_obj_area, new_bg_area, new_obj_bbox, alpha=alpha)\n",
    "    return model\n",
    "\n",
    "# ========================================================\n",
    "# Example Usage\n",
    "# ========================================================\n",
    "if __name__ == '__main__':\n",
    "    # Assume we have a video file or camera feed.\n",
    "    cap = cv2.VideoCapture('red_dot_video.mp4')  # Change to 0 for webcam\n",
    "\n",
    "    # Read the first frame to initialize the model.\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture the first frame.\")\n",
    "        cap.release()\n",
    "        exit(0)\n",
    "\n",
    "    # Initialize the segmentation model on the first frame.\n",
    "    model = initialize_segmentation_model(frame, k=2)\n",
    "    print(\"Initial model:\")\n",
    "    print(\"  Object LAB:\", model.object_color_lab)\n",
    "    print(\"  Background LAB:\", model.background_color_lab)\n",
    "    print(\"  Object area:\", model.object_area)\n",
    "    print(\"  Object bbox:\", model.object_bbox)\n",
    "    \n",
    "    # Save the initial model for future use.\n",
    "    model.save('segmentation_model.pkl')\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        #plt.imshow(frame)\n",
    "        #plt.show()  \n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "        # (Optionally) Load the model from disk if needed:\n",
    "        # model = SegmentationModel.load('segmentation_model.pkl')\n",
    "\n",
    "        # Update the model using the current frame.\n",
    "        #model = update_segmentation_model(frame, model, k=2, alpha=0.1)\n",
    "        # Print current model parameters.\n",
    "        #print(\"Updated model:\")\n",
    "        #print(\"  Object LAB:\", model.object_color_lab)\n",
    "\n",
    "        gf = mask_pixels_within_lab_range(frame, [(125.487305, 202.85654,  190.72),], tolerance=0.1)\n",
    "        #plt.imshow(gf)\n",
    "        #plt.show()  \n",
    "        # (Optional) Display the updated object mask on the current frame.\n",
    "        #result = segment_two_colors(frame, k=2)\n",
    "        #plt.imshow(result['object_mask'])\n",
    "        #plt.show()\n",
    "\n",
    "    # Save the updated model at the end.\n",
    "    model.save('segmentation_model.pkl')\n",
    "    print(\"Final model saved.\")\n",
    "    cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
